{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install networkx\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install graphviz\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, groupby\n",
    "\n",
    "from networkx.algorithms import tree\n",
    "from networkx.algorithms import bellman_ford_predecessor_and_distance\n",
    "from networkx.algorithms import floyd_warshall_predecessor_and_distance\n",
    "\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Algorithm's analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can use this function to generate a random graph with 'num_of_nodes' nodes\n",
    "# and 'completeness' probability of an edge between any two nodes\n",
    "# If 'directed' is True, the graph will be directed\n",
    "# If 'draw' is True, the graph will be drawn\n",
    "def gnp_random_connected_graph(num_of_nodes: int,\n",
    "                               completeness: int,\n",
    "                               directed: bool = False,\n",
    "                               draw: bool = False):\n",
    "    \"\"\"\n",
    "    Generates a random graph, similarly to an Erdős-Rényi\n",
    "    graph, but enforcing that the resulting graph is conneted (in case of undirected graphs)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if directed:\n",
    "        G = nx.DiGraph()\n",
    "    else:\n",
    "        G = nx.Graph()\n",
    "    edges = combinations(range(num_of_nodes), 2)\n",
    "    G.add_nodes_from(range(num_of_nodes))\n",
    "\n",
    "    for _, node_edges in groupby(edges, key = lambda x: x[0]):\n",
    "        node_edges = list(node_edges)\n",
    "        random_edge = random.choice(node_edges)\n",
    "        if random.random() < 0.5:\n",
    "            random_edge = random_edge[::-1]\n",
    "        G.add_edge(*random_edge)\n",
    "        for e in node_edges:\n",
    "            if random.random() < completeness:\n",
    "                G.add_edge(*e)\n",
    "\n",
    "    for (u,v,w) in G.edges(data=True):\n",
    "        w['weight'] = random.randint(-5, 20)\n",
    "\n",
    "    if draw:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        if directed:\n",
    "            # draw with edge weights\n",
    "            pos = nx.arf_layout(G)\n",
    "            nx.draw(G,pos, node_color='lightblue',\n",
    "                    with_labels=True,\n",
    "                    node_size=500,\n",
    "                    arrowsize=20,\n",
    "                    arrows=True)\n",
    "            labels = nx.get_edge_attributes(G,'weight')\n",
    "            nx.draw_networkx_edge_labels(G, pos,edge_labels=labels)\n",
    "\n",
    "        else:\n",
    "            nx.draw(G, node_color='lightblue',\n",
    "                with_labels=True,\n",
    "                node_size=500)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gnp_random_connected_graph(10, 0.2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kruskal's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstk = tree.minimum_spanning_tree(G, algorithm=\"kruskal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(mstk, node_color='lightblue',\n",
    "        with_labels=True,\n",
    "        node_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, node_color='lightblue',\n",
    "                with_labels=True,\n",
    "                node_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstk.edges().data(\"weight\"), sum(w for _, _, w in mstk.edges().data(\"weight\")), len(mstk.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Kruskal_algorithm(G: nx.Graph) -> nx.Graph:\n",
    "    tr: nx.Graph = nx.create_empty_copy(G)\n",
    "    n = len(G.nodes())\n",
    "    # print(n)\n",
    "    edges = G.edges().data(\"weight\")\n",
    "    edges = sorted(edges, key=lambda x: x[2])\n",
    "\n",
    "    for u, v, w in edges:\n",
    "        if not nx.has_path(tr, u, v):\n",
    "            tr.add_edge(u, v, weight=w)\n",
    "\n",
    "    return tr\n",
    "\n",
    "tr = Kruskal_algorithm(G)\n",
    "nx.draw(tr, node_color='lightblue',\n",
    "        with_labels=True,\n",
    "        node_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.edges().data(\"weight\"), sum(w for _, _, w in tr.edges().data(\"weight\")), len(tr.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prim's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstp = tree.minimum_spanning_tree(G, algorithm=\"prim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(mstp, node_color='lightblue',\n",
    "        with_labels=True,\n",
    "        node_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mstp.edges().data(\"weight\"), sum(w for _, _, w in mstp.edges().data(\"weight\")), len(mstp.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prim_algorithm(G: nx.Graph) -> nx.Graph:\n",
    "    tr = nx.create_empty_copy(G)\n",
    "\n",
    "    adj_list = dict.fromkeys(G.nodes(), None)\n",
    "    for (u, v, wt) in G.edges.data('weight'):\n",
    "        if not adj_list[u]:\n",
    "            adj_list[u] = {}\n",
    "        if not adj_list[v]:\n",
    "            adj_list[v] = {}\n",
    "        adj_list[u][v] = wt\n",
    "        adj_list[v][u] = wt\n",
    "    #     print(u, v, wt)\n",
    "    # print(adj_list)\n",
    "\n",
    "    v = list(G.nodes())[0]\n",
    "    pr_q = []\n",
    "    for to, w in adj_list[v].items():\n",
    "        heapq.heappush(pr_q, (w, to, v))\n",
    "    connected = set()\n",
    "    connected.add(v)\n",
    "    while pr_q != []:\n",
    "        weight, v, prev = heapq.heappop(pr_q)\n",
    "        if v in connected:\n",
    "            continue\n",
    "        connected.add(v)\n",
    "        tr.add_edge(prev, v, weight=weight)\n",
    "\n",
    "        for to, w in adj_list[v].items():\n",
    "            heapq.heappush(pr_q, (w, to, v))\n",
    "    return tr\n",
    "\n",
    "tr = Prim_algorithm(G)\n",
    "nx.draw(tr, node_color='lightblue',\n",
    "        with_labels=True,\n",
    "        node_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.edges().data(\"weight\"), sum(w for _, _, w in tr.edges().data(\"weight\")), len(tr.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gnp_random_connected_graph(10, 0.5, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman-Ford algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred is a dictionary of predecessors, dist is a dictionary of distances\n",
    "try:\n",
    "    pred, dist = bellman_ford_predecessor_and_distance(G, 0)\n",
    "    for k, v in dist.items():\n",
    "        print(f\"Distance to {k}:\", v)\n",
    "except:\n",
    "    print(\"Negative cycle detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_ford_algorithm(G, start_node: int):\n",
    "    dist = dict.fromkeys(G.nodes(), float('inf'))\n",
    "    dist[start_node] = 0\n",
    "\n",
    "    for _ in range(len(G.nodes()) - 1):\n",
    "        for u, v, weight in G.edges(data=True):\n",
    "            if dist[u] != float('inf') and dist[u] + weight['weight'] < dist[v]:\n",
    "                dist[v] = dist[u] + weight['weight']\n",
    "\n",
    "    # check for negative weight cycles\n",
    "    for u, v, weight in G.edges(data=True):\n",
    "        if dist[u] != float('inf') and dist[u] + weight['weight'] < dist[v]:\n",
    "            return \"Negative cycle detected\"\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bellman_ford_algorithm(G, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floyd-Warshall algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred is a dictionary of predecessors, dist is a dictionary of distances dictionaries\n",
    "try:\n",
    "    pred, dist = floyd_warshall_predecessor_and_distance(G)\n",
    "    for k, v in dist.items():\n",
    "        print(f\"Distances with {k} source:\", dict(v))\n",
    "except:\n",
    "    print(\"Negative cycle detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floyd_warshall_algorithm(G):\n",
    "    n = len(G.nodes())\n",
    "    dist = [[float('inf') for _ in range(n)] for _ in range(n)]\n",
    "\n",
    "    # set distance from node to itself to zero\n",
    "    for i in range(n):\n",
    "        dist[i][i] = 0\n",
    "\n",
    "    # set initial distances\n",
    "    for u, v, weight in G.edges(data=True):\n",
    "        dist[u][v] = weight['weight']\n",
    "\n",
    "    for k in range(n):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                dist[i][j] = min(dist[i][j], (dist[i][k] + dist[k][j]))\n",
    "\n",
    "    # check for negative weight cycles\n",
    "    for i in range(n):\n",
    "        if dist[i][i] < 0:\n",
    "            return \"Negative cycle detected\"\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_w = floyd_warshall_algorithm(G)\n",
    "fl_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fl_w)):\n",
    "    print(f\"Distances with {i} source: { {k: dist for k, dist in zip(range(len(fl_w)), fl_w[i])} }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful explanations\n",
    "### How to get list of edges for your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(G.edges()) # by default G.edges are EdgesView class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get edges with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = list(G.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time measuring\n",
    "\n",
    "Read more on this: https://realpython.com/python-timer/\n",
    "\n",
    "Recall that you should measure times for 5, 10, 20, 50, 100, 200, 500 nodes 1000 times (and take mean of time taken for each node amount).\n",
    "\n",
    "Then you should build the plot for two algorithms (x - data size, y - mean time of execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_ITERATIONS = 1000\n",
    "num_nodes = [5, 10, 20, 50, 100, 200, 500]\n",
    "completenesses = [0, 0.2, 0.4, 0.5, 0.75, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum spanning tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measures_MST = {\n",
    "#     \"nx_prim\": {\n",
    "#         \"time\": {},\n",
    "#         \"alg\": lambda g: tree.minimum_spanning_tree(g, algorithm=\"prim\")\n",
    "#     },\n",
    "#     \"my_prim\": {\n",
    "#         \"time\": {},\n",
    "#         \"alg\": lambda g: Prim_algorithm(g)\n",
    "#     },\n",
    "#     \"nx_krustal\": {\n",
    "#         \"time\": {},\n",
    "#         \"alg\": lambda g: tree.minimum_spanning_tree(g, algorithm=\"kruskal\")\n",
    "#     },\n",
    "#     \"my_krustal\": {\n",
    "#         \"time\": {},\n",
    "#         \"alg\": lambda g: Kruskal_algorithm(g)\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# for n in num_nodes:\n",
    "#     for k in measures_MST:\n",
    "#         measures_MST[k][\"time\"][n] = 0\n",
    "\n",
    "#     for i in tqdm(range(NUM_OF_ITERATIONS)):\n",
    "\n",
    "#         # note that we should not measure time of graph creation\n",
    "#         G = gnp_random_connected_graph(n, completenesses[i%len(completenesses)], False)\n",
    "\n",
    "#         for k in measures_MST:\n",
    "#             start = time.time()\n",
    "#             gr = measures_MST[k][\"alg\"](G)\n",
    "#             end = time.time()\n",
    "\n",
    "#             measures_MST[k][\"time\"][n] += end - start\n",
    "\n",
    "#     for k in measures_MST:\n",
    "#         measures_MST[k][\"time\"][n] = measures_MST[k][\"time\"][n] / NUM_OF_ITERATIONS\n",
    "# measures_MST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting time measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum spanning tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this if you don't want to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: number of nodes and measured times for each algorithm.\n",
    "num_nodes = [5, 10, 20, 50, 100, 200, 500]\n",
    "\n",
    "measures_MST = {\n",
    "    'nx_prim': {\n",
    "        'time': {\n",
    "            5: 3.6245107650756835e-05,\n",
    "            10: 0.00013869738578796387,\n",
    "            20: 0.0001194601058959961,\n",
    "            50: 0.000502967119216919,\n",
    "            100: 0.00245829701423645,\n",
    "            200: 0.015145911455154419,\n",
    "            500: 0.11887391948699952\n",
    "        },\n",
    "        'alg': lambda g: None  # placeholder function\n",
    "    },\n",
    "    'my_prim': {\n",
    "        'time': {\n",
    "            5: 2.8157472610473633e-05,\n",
    "            10: 6.421422958374024e-05,\n",
    "            20: 0.00015464949607849122,\n",
    "            50: 0.0008819258213043213,\n",
    "            100: 0.005444227933883667,\n",
    "            200: 0.030584785461425783,\n",
    "            500: 0.2457535240650177\n",
    "        },\n",
    "        'alg': lambda g: None\n",
    "    },\n",
    "    'nx_krustal': {\n",
    "        'time': {\n",
    "            5: 3.651738166809082e-05,\n",
    "            10: 7.623982429504395e-05,\n",
    "            20: 0.00015038323402404786,\n",
    "            50: 0.0006659302711486816,\n",
    "            100: 0.003551886320114136,\n",
    "            200: 0.018301385402679442,\n",
    "            500: 0.15191850209236146\n",
    "        },\n",
    "        'alg': lambda g: None\n",
    "    },\n",
    "    'my_krustal': {\n",
    "        'time': {\n",
    "            5: 5.860638618469238e-05,\n",
    "            10: 9.623982429504395e-05,\n",
    "            20: 0.00095038323402404786,\n",
    "            50: 0.005616315841674805,\n",
    "            100: 0.048421209573745724,\n",
    "            200: 0.3302415544986725,\n",
    "            500: 3.673677587032318\n",
    "        },\n",
    "        'alg': lambda g: None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute time measures limited to 1 second\n",
    "\n",
    "my_kruskal takes an average of 3.673677587032318 seconds for 500 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract algorithm names.\n",
    "algorithms = list(measures_MST.keys())\n",
    "\n",
    "# Prepare the times for each algorithm in the order of num_nodes.\n",
    "times = {\n",
    "    alg: [measures_MST[alg]['time'][n] for n in num_nodes]\n",
    "    for alg in algorithms\n",
    "}\n",
    "\n",
    "# Set up the bar plot parameters.\n",
    "x = np.arange(len(num_nodes))  # positions for groups (each group corresponds to a graph size)\n",
    "bar_width = 0.2  # width of each individual bar\n",
    "offset = -bar_width * (len(algorithms) - 1) / 2  # centering the grouped bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create a bar for each algorithm.\n",
    "for i, alg in enumerate(algorithms):\n",
    "    ax.bar(x + offset + i * bar_width, times[alg],\n",
    "           width=bar_width, label=alg)\n",
    "\n",
    "# Set the labels and title.\n",
    "ax.set_xlabel(\"Number of Nodes\")\n",
    "ax.set_ylabel(\"Time (seconds)\")\n",
    "ax.set_title(\"Execution Time of Different MST Algorithms\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(num_nodes)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Display the plot.\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped by number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of algorithm names.\n",
    "algorithms = list(measures_MST.keys())\n",
    "\n",
    "# Decide on a grid layout for subplots.\n",
    "n_plots = len(num_nodes)\n",
    "n_cols = 4                     # Number of subplot columns (you can adjust this)\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols  # Compute the number of rows needed\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "axes = axes.flatten()  # Flatten in case we have a 2D array of axes\n",
    "\n",
    "# Create a bar plot for each graph size.\n",
    "for i, nodes in enumerate(num_nodes):\n",
    "    # Extract times for the current number of nodes for each algorithm.\n",
    "    times = [measures_MST[alg]['time'][nodes] for alg in algorithms]\n",
    "\n",
    "    # Plot bars for each algorithm.\n",
    "    axes[i].bar(algorithms, times, color='skyblue')\n",
    "\n",
    "    # Set title and labels.\n",
    "    axes[i].set_title(f\"Graph Size: {nodes} Nodes\")\n",
    "    axes[i].set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "    # Optionally, adjust y-axis limits so the bars are clearly visible.\n",
    "    max_time = max(times)\n",
    "    axes[i].set_ylim(0, max_time * 1.2)  # a little extra space above the tallest bar\n",
    "\n",
    "    # Rotate x-labels if necessary for better readability.\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# If there are any unused subplots, remove them.\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped by number of nodes without my_kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of algorithm names.\n",
    "algorithms = list(measures_MST.keys())[:-1]\n",
    "\n",
    "# Decide on a grid layout for subplots.\n",
    "n_plots = len(num_nodes)\n",
    "n_cols = 3                     # Number of subplot columns (you can adjust this)\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols  # Compute the number of rows needed\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "axes = axes.flatten()  # Flatten in case we have a 2D array of axes\n",
    "\n",
    "# Create a bar plot for each graph size.\n",
    "for i, nodes in enumerate(num_nodes):\n",
    "    # Extract times for the current number of nodes for each algorithm.\n",
    "    times = [measures_MST[alg]['time'][nodes] for alg in algorithms]\n",
    "\n",
    "    # Plot bars for each algorithm.\n",
    "    axes[i].bar(algorithms, times, color='skyblue')\n",
    "\n",
    "    # Set title and labels.\n",
    "    axes[i].set_title(f\"Graph Size: {nodes} Nodes\")\n",
    "    axes[i].set_ylabel(\"Time (seconds)\")\n",
    "\n",
    "    # Optionally, adjust y-axis limits so the bars are clearly visible.\n",
    "    max_time = max(times)\n",
    "    axes[i].set_ylim(0, max_time * 1.2)  # a little extra space above the tallest bar\n",
    "\n",
    "    # Rotate x-labels if necessary for better readability.\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# If there are any unused subplots, remove them.\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn package\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General idea\n",
    "\n",
    "#### You are expected to write a quite simple, yet good core logic of decision tree classifier class. Additionaly, you need to test your results and write down a report on what you've done, which principles used and explain the general process.\n",
    "\n",
    "#### Hopefully, you have already learned what is decision tree classifier and how it work. For better understanding, and in case if something is still unclear for you, here are some useful links on basics of DTC:\n",
    "- https://www.youtube.com/watch?v=ZVR2Way4nwQ\n",
    "- https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e\n",
    "- https://towardsdatascience.com/decision-tree-from-scratch-in-python-46e99dfea775\n",
    "- https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial\n",
    "- https://towardsdatascience.com/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b6057\n",
    "\n",
    "#### Also, for those interested to learn more about machine learning and particulary Desicion Trees - here is a great course on Coursera (you may be interested in the whole course or just this particular week):\n",
    "- https://www.coursera.org/learn/advanced-learning-algorithms/home/week/4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n",
    "#### You can use Iris dataset for this task. It is a very popular dataset for machine learning and data science. It contains 150 samples of 3 different species of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. \n",
    "Read more on this: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "#### However, using more interesting and intricate datasets is much appreciated. You can use any dataset you want, but it should be a classification one. For example you can use breast cancer or wine datasets, which are also available in sklearn.datasets. Or you can use any other dataset you find interesting.\n",
    "P.S. In case you are not sure if your dataset is suitable, feel free to ask assistants :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "dir(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we have 150 entries (samples, infos about a flower). The columns being: Sepal Length, Sepal Width, Petal Length and Petal Width(features). Let's look at first two entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To undestand data little bit better, let's plot some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "plt.figure(2, figsize=(8, 6))\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor=\"k\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Sepal width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can clearly see, that even basing on those two parameters, we can clearly divide (classify) out data into several groups. For this, we will use decision tree classifier: https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n",
    "\n",
    "### Example of usage\n",
    "\n",
    "\n",
    "**Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression**. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / test split\n",
    "\n",
    "We train our model using training dataset and evaluate its performance basing on the test dataset. Reason to use two separate datasets is that our model learns its parameters from data, thus test set allows us to check its possibilities on completely new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(X, y, test_size= 0.20)\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model learning\n",
    "\n",
    "It learns its parameters (where it should split data and for what threshold value) basing on the training dataset. It is done by minimizing some cost function (e.g. Gini impurity or entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of produced tree\n",
    "\n",
    "You do not need to understand this piece of code :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                     feature_names=iris.feature_names,\n",
    "                     class_names=iris.target_names,\n",
    "                     filled=True, rounded=True,\n",
    "                     special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction step\n",
    "\n",
    "Now we can use our model to predict which type has a flower, basing on its parameters.\n",
    "\n",
    "This is conducted basically via traversing the tree that you can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also measure the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predictions == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get clearer intuition about predicion, let's look at those X, that should be labeled to some flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here you can traverse the tree above by yourself and make sure that prediction works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([X_test[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, it is your turn to write such classifier by yourself!\n",
    "\n",
    "####  Gini impurity\n",
    "\n",
    "Decision trees use the concept of Gini impurity to describe how “pure” a node is. A node is pure (G = 0) if all its samples belong to the same class, while a node with many samples from many different classes will have a Gini closer to 1.\n",
    "\n",
    "$G = 1 - \\sum_{k=1}^{n}p_{k}^2$\n",
    "\n",
    "For example, if a node contains five samples, with two belonging to the first class (first flower), two of class 2, one of class 3 and none of class 4, then\n",
    "\n",
    "$G = 1 - (\\frac{2}{5})^2 - (\\frac{2}{5})^2 - (\\frac{1}{5})^2 = 0.64$\n",
    "\n",
    "#### Remarks \n",
    "- We recommend using additional functions in `DecisionTreeClassifier` class, to make the implementation process easier.\n",
    "- [use this hint](https://arc.net/l/quote/pqvyjqei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, X: npt.NDArray, y: npt.NDArray, laplas_smoothing: bool=True, alpha: float=0, d: int=0):\n",
    "        \"\"\"\n",
    "        :param X: numpy array of form [[feature1,feature2, ... featureN], ...] (i.e. [[1.5, 5.4, 3.2, 9.8] , ...] for case with iris d.s.)\n",
    "        :param y: numpy array of from [class1, class2, ...] (i.e. [0,1,1,2,1,0,...] for case with iris d.s.)\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.gini = self.calculate_gini_laplas(y, alpha, d) if laplas_smoothing else self.calculate_gini(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_gini(y: npt.NDArray) -> float:\n",
    "        _, counts = np.unique_counts(y)\n",
    "        total = np.sum(counts)\n",
    "        return 1 - np.sum((counts/total)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_gini_laplas(y: npt.NDArray, alpha: float, d: int) -> float:\n",
    "        _, counts = np.unique_counts(y)\n",
    "        total = np.sum(counts)\n",
    "        return 1 - np.sum( ((counts+alpha)/(total+alpha*d)) ** 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self, max_depth: int) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "        self.number_of_classes = None\n",
    "\n",
    "\n",
    "    def fit(self, X: npt.NDArray, y: npt.NDArray, alpha: float=1) -> None:\n",
    "        \"\"\"\n",
    "        Basically, function that performs all the training (building of a tree)\n",
    "        We recommend to use it as a wrapper of recursive building function\n",
    "        \"\"\"\n",
    "        self.number_of_classes = np.unique(y).size\n",
    "        self.tree = Node(X, y, True, alpha, self.number_of_classes)\n",
    "\n",
    "        features_count = X.shape[1]\n",
    "        uniques = [np.unique(X[:, f_i]) for f_i in range(features_count)]\n",
    "\n",
    "\n",
    "        def inner(root: Node, depth: int):\n",
    "            if depth >= self.max_depth:\n",
    "                return\n",
    "\n",
    "            max_info_gain = 0\n",
    "            for feature_index in range(features_count):\n",
    "                for threshold in uniques[feature_index]:\n",
    "                    left_data, right_data = self.split_data(root.X, root.y, feature_index, threshold)\n",
    "                    if left_data[0].size == 0 or right_data[0].size == 0:\n",
    "                        continue\n",
    "\n",
    "                    left = Node(left_data[0], left_data[1], True, alpha, self.number_of_classes)\n",
    "                    right = Node(right_data[0], right_data[1], True, alpha, self.number_of_classes)\n",
    "\n",
    "                    #maximize information gain\n",
    "                    left_info = left.gini * left.y.size / root.y.size\n",
    "                    right_info = right.gini * right.y.size / root.y.size\n",
    "                    cur_info_gain = root.gini - left_info - right_info\n",
    "                    if cur_info_gain > max_info_gain:\n",
    "                        max_info_gain = cur_info_gain\n",
    "                        root.feature_index = feature_index\n",
    "                        root.threshold = threshold\n",
    "                        root.left = left\n",
    "                        root.right = right\n",
    "\n",
    "            if root.left is None:\n",
    "                return\n",
    "            if root.left.gini != 0:\n",
    "                inner(root.left, depth+1)\n",
    "            if root.right.gini != 0:\n",
    "                inner(root.right, depth+1)\n",
    "\n",
    "        inner(self.tree, 0)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def split_data(X: npt.NDArray, y: npt.NDArray, feature_index: int, threshold) \\\n",
    "        -> tuple[tuple[npt.NDArray, npt.NDArray], tuple[npt.NDArray, npt.NDArray]]:\n",
    "        left, right = ([], []), ([], [])\n",
    "        for i in range(X.shape[0]):\n",
    "            if X[i, feature_index] <= threshold:\n",
    "                left[0].append(X[i])\n",
    "                left[1].append(y[i])\n",
    "            else:\n",
    "                right[0].append(X[i])\n",
    "                right[1].append(y[i])\n",
    "        return (np.array(left[0]), np.array(left[1])), (np.array(right[0]), np.array(right[1]))\n",
    "\n",
    "    def predict(self, X_test: npt.NDArray) -> list:\n",
    "        \"\"\"\n",
    "        Traverse the tree while there is a child\n",
    "        and return the predicted class for it\n",
    "        \"\"\"\n",
    "\n",
    "        def classify(node: Node, X: npt.ArrayLike) -> int:\n",
    "            if node.left is None:\n",
    "                ch, w = np.unique_counts(node.y)\n",
    "                return random.choices(ch, weights=w)[0]\n",
    "\n",
    "            if X[node.feature_index] <= node.threshold:\n",
    "                return classify(node.left, X)\n",
    "            return classify(node.right, X)\n",
    "\n",
    "        predictions = []\n",
    "        for X in X_test:\n",
    "            predictions.append(classify(self.tree, X))\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: MyDecisionTreeClassifier, X_test: list[list], y_test: list) -> float:\n",
    "    \"\"\"\n",
    "    Returns accuracy of the model (ratio of right guesses to the number of samples)\n",
    "    \"\"\"\n",
    "    my_predictions = model.predict(X_test)\n",
    "    return float(sum(my_predictions == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(5)\n",
    "my_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(my_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf.fit(X, y, 0.8)\n",
    "evaluate(my_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf.fit(X, y, 0.2)\n",
    "evaluate(my_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf.fit(X, y, 2)\n",
    "evaluate(my_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf.fit(X, y, 5)\n",
    "evaluate(my_clf, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
